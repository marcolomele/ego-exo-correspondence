"""
Evaluation script for O-MaMa using precomputed DINOV2, DINOv3 or ResNet-50 features.

This is a modified version of main_eval.py that:
- Uses precomputed features loaded from disk (3-5x faster evaluation)
- Uses DescriptorExtractorPrecomputed (no DINO model)
- Uses Masks_Dataset_Precomputed (loads .npz features)
- Supports SEPARATE checkpoints for O-MaMa model and projection layer

Prerequisites:
    - Precomputed features generated by precompute_features.py
    - O-MaMa model checkpoint (baseline or trained)
    - (Optional) Separate projection layer checkpoint

Usage for DINOV2 features:
    python main_eval_precomputed.py --root ../../data/root --reverse --checkpoint_dir path/to/checkpoint.pt --patch_size 14 --dino_feat_dim 768 --checkpoint_dir train_output/run_XXX/model_weights/best_IoU_run_XXX.pt
    
Usage for DINOv3 features:
    python main_eval_precomputed.py --root ../../data/root --reverse --checkpoint_dir path/to/checkpoint.pt --patch_size 16 --checkpoint_dir train_output/run_XXX/model_weights/best_IoU_run_XXX.pt

Usage for ResNet-50 features:
    python main_eval_precomputed.py --root ../../data/root --reverse --checkpoint_dir path/to/checkpoint.pt --dino_feat_dim 2048 --checkpoint_dir train_output/run_XXX/model_weights/best_IoU_run_XXX.pt

Usage examples:
    # Evaluate with combined checkpoint (both model + projection in one file)
    python main_eval_precomputed.py --root ../../data/root --reverse --patch_size 16 \
        --checkpoint_dir train_output/run_XXX/model_weights/best_IoU_run_XXX.pt
    
    # Evaluate with separate checkpoints (e.g., baseline O-MaMa weights + trained projection)
    python main_eval_precomputed.py --root ../../data/root --reverse --patch_size 16 \
        --checkpoint_dir Exo2Ego_weights_checkpoint.pt \
        --projection_checkpoint train_output/run_YYY/projection_weights/best_IoU_run_YYY.pt

"""

import torch
import argparse
import numpy as np
import json
from descriptors.get_descriptors_precomputed import DescriptorExtractorPrecomputed
from dataset.dataset_masks_precomputed import Masks_Dataset_Precomputed
from model.model import Attention_projector
from evaluation.evaluate import add_to_json, evaluate
from pathlib import Path

import helpers
from datetime import datetime
from tqdm import tqdm
import os
import sys
import logging


def save_json(data, path, description="JSON"):
    """
    Safely save JSON data with error handling and verification.
    """
    try:
        os.makedirs(os.path.dirname(path), exist_ok=True)
        
        with open(path, 'w') as f:
            json.dump(data, f, indent=4)
            f.flush()
            os.fsync(f.fileno())
        
        if not os.path.exists(path):
            logging.error(f"Failed to save {description}: File does not exist after save attempt: {path}")
            return False
        
        file_size = os.path.getsize(path)
        if file_size == 0:
            logging.error(f"Failed to save {description}: File is empty: {path}")
            return False
        
        logging.info(f"Successfully saved {description} to {path} (size: {file_size / 1024:.2f} KB)")
        return True
    
    except Exception as e:
        logging.error(f"Error saving {description} to {path}: {str(e)}")
        return False


def compute_IoU(pred_mask, gt_mask):
    intersection = torch.logical_and(pred_mask, gt_mask).sum()
    union = torch.logical_or(pred_mask, gt_mask).sum()
    IoU = intersection / (union + 1e-6)
    return IoU


def convert_ndarray(obj):
    if isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {k: convert_ndarray(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_ndarray(item) for item in obj]
    else:
        return obj


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Evaluate O-MaMa with precomputed DINOv3 features")
    parser.add_argument("--root", type=str, default="../../data/root", help="Path to the dataset")
    parser.add_argument("--features_dir", type=str, default="../../data/root/precomputed_features", help="Path to precomputed features")
    parser.add_argument("--output_dir", type=str, default="eval_output", help="Output directory")
    
    parser.add_argument("--projection_checkpoint", type=str, default=None, help="Path to projection layer checkpoint (optional, separate from model)")
    parser.add_argument("--checkpoint_dir", type=str, default="Exo2Ego_weights_checkpoint.pt")
    
    parser.add_argument("--reverse", action="store_true", help="Flag to select exo->ego pairs")
    
    parser.add_argument("--order", default=2, type=int, help="order of adjacency matrix, 2 for 2nd order")
    parser.add_argument("--devices", default="0", type=str)
    parser.add_argument("--patch_size", type=int, default=32, help="Patch size of the dino transformer")
    parser.add_argument("--context_size", type=int, default=20, help="Size of the context for the object")
    parser.add_argument("--N_masks_per_batch", default=16, type=int)
    
    parser.add_argument("--exp_name", type=str, default="Evaluation_OMAMA_Precomputed")
    parser.add_argument("--num_workers", type=int, default=4, help="Number of data loading workers (default: 0)")
    parser.add_argument("--percent_test", type=float, default=1.0, help="Percentage of test set to use (0.0-1.0, default: 1.0)")
    parser.add_argument("--dino_feat_dim", type=int, default=768, help="Extractor model feature dimension (default: 768 for DINOv2)")
    parser.add_argument("--test_on_subset", type=int, default=None, help="Limit the test dataset to the first N items")
    parser.add_argument("--target_feat_dim", type=int, default=768, help="Target feature dimension (default: 768)")
    args = parser.parse_args()

    # Setup logging to both console and file
    now = datetime.now()
    run_folder = f"run_{now.strftime('%Y%m%d')}_{now.strftime('%H%M%S')}"
    
    if args.output_dir is not None:
        base_output_dir = Path(args.output_dir).resolve()
    else:
        script_dir = Path(__file__).parent.resolve()
        base_output_dir = script_dir / "eval_output"
    
    output_dir = base_output_dir / run_folder
    
    try:
        output_dir.mkdir(parents=True, exist_ok=True)
        print(f"Created output directory: {output_dir}")
    except Exception as e:
        print(f"ERROR: Failed to create output directories: {e}")
        print(f"Attempted path: {output_dir}")
        sys.exit(1)
    
    log_file = output_dir / f"evaluation_{run_folder}.log"
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler(sys.stdout)
        ]
    )
    
    logging.info(f"Starting evaluation run: {run_folder}")
    logging.info(f"Output directory: {output_dir}")
    logging.info(f"Command line arguments: {vars(args)}")
    logging.info("=" * 60)
    logging.info("USING PRECOMPUTED FEATURES PIPELINE")
    logging.info("=" * 60)
    
    # Verify output directory is writable
    test_file = output_dir / ".write_test"
    try:
        test_file.write_text("test")
        test_file.unlink()
        logging.info("Output directory is writable")
    except Exception as e:
        logging.error(f"Output directory is NOT writable: {e}")
        sys.exit(1)

    helpers.set_all_seeds(42)
    if args.devices != "cpu":
        gpus = [args.devices]
        device_ids = [f'cuda:{gpu}' for gpu in gpus]
        device = torch.device(f'cuda:{device_ids[0].split(":")[1]}') if torch.cuda.is_available() else 'cpu'
    else:
        device = 'cpu'
    
    logging.info(f"Using device: {device}")

    # Determine features directory
    features_dir = args.features_dir if args.features_dir else os.path.join(args.root, 'precomputed_features')
    logging.info(f"Loading precomputed features from: {features_dir}")

    # Test dataset with precomputed features
    logging.info("Loading test dataset (precomputed features)...")
    test_dataset = Masks_Dataset_Precomputed(
        args.root, args.patch_size, args.reverse, 
        train=False, N_masks_per_batch=args.N_masks_per_batch, 
        order=args.order, test=True,
        features_dir=features_dir
    )
    
    # Apply percent_test sampling if specified
    if args.percent_test < 1.0:
        import random
        random.seed(42)  # For reproducibility
        
        total_samples = len(test_dataset)
        num_samples = int(total_samples * args.percent_test)
        
        # Randomly sample indices
        sampled_indices = random.sample(range(total_samples), num_samples)
        sampled_indices.sort()  # Keep them sorted for consistency
        
        from torch.utils.data import Subset
        test_dataset_sampled = Subset(test_dataset, sampled_indices)
        logging.info(f"Randomly sampled {args.percent_test*100:.1f}% of test set: {num_samples}/{total_samples} samples")
        
        # Store reference to original dataset for add_to_json
        test_dataset_for_eval = test_dataset
        test_dataset = test_dataset_sampled
    else:
        test_dataset_for_eval = test_dataset
    
    if args.test_on_subset is not None:
        limited_indices = list(range(min(args.test_on_subset, len(test_dataset))))
        from torch.utils.data import Subset
        test_dataset_limited = Subset(test_dataset, limited_indices)
        logging.info(f"Limiting test dataset to first {args.test_on_subset} items for evaluation.")
        test_dataloader = torch.utils.data.DataLoader(
            test_dataset_limited, batch_size=1, shuffle=False, 
            collate_fn=helpers.our_collate_fn, 
            num_workers=args.num_workers, 
            pin_memory=True
        )
        logging.info(f"Test dataset loaded: {len(test_dataset_limited)} samples (limited from {len(test_dataset)})")
        # Use the base dataset (before percent_test sampling) for add_to_json
        if not hasattr(test_dataset, 'mask_annotations'):
            test_dataset_for_eval = test_dataset.dataset if hasattr(test_dataset, 'dataset') else test_dataset
    else:
        test_dataloader = torch.utils.data.DataLoader(
            test_dataset, batch_size=1, shuffle=False, 
            collate_fn=helpers.our_collate_fn, 
            num_workers=args.num_workers, 
            pin_memory=True
        )
        logging.info(f"Test dataset loaded: {len(test_dataset)} samples")
    
    logging.info("Initializing descriptor extractor and model...")
    
    # Use precomputed descriptor extractor (no DINO model!)
    descriptor_extractor = DescriptorExtractorPrecomputed(
        args.patch_size, args.context_size, device,
        dino_feat_dim=args.dino_feat_dim,
        target_feat_dim=args.target_feat_dim
    )
    logging.info(f"Descriptor extractor initialized (projection: {args.dino_feat_dim} -> {args.target_feat_dim})")
    
    model = Attention_projector(args.reverse).to(device)
    logging.info(f"Model:\n{model}")

    logging.info(f"Loading checkpoint from: {args.checkpoint_dir}")
    try:
        checkpoint = torch.load(args.checkpoint_dir, map_location=device, weights_only=False)
        
        # Handle both old and new checkpoint formats
        if 'model_state_dict' in checkpoint:
            state_dict = checkpoint['model_state_dict']
        else:
            # Old format - just the model state dict
            state_dict = checkpoint
        
        # Resize position embeddings if there's a size mismatch
        model_state = model.state_dict()
        for key in ['pos_embed_T', 'pos_embed_Q']:
            if key in state_dict and key in model_state:
                checkpoint_shape = state_dict[key].shape
                model_shape = model_state[key].shape
                
                if checkpoint_shape != model_shape:
                    logging.warning(f"Resizing {key} from {checkpoint_shape} to {model_shape}")
                    # Interpolate position embeddings
                    old_embed = state_dict[key]  # [1, old_num_patches, dim]
                    
                    # Extract spatial dimensions
                    old_num_patches = checkpoint_shape[1]
                    new_num_patches = model_shape[1]
                    embed_dim = checkpoint_shape[2]
                    
                    # Calculate actual grid dimensions
                    # Try to find best factorization for old_num_patches
                    old_h = int(old_num_patches ** 0.5)
                    old_w = old_num_patches // old_h
                    
                    # Adjust if not perfect
                    while old_h * old_w != old_num_patches:
                        old_h -= 1
                        old_w = old_num_patches // old_h
                    
                    new_h = new_w = int(new_num_patches ** 0.5)
                    
                    logging.info(f"  Old grid: {old_h}x{old_w}={old_num_patches}, New grid: {new_h}x{new_w}={new_num_patches}")
                    
                    # Reshape to spatial grid: [1, old_h, old_w, dim]
                    old_embed_spatial = old_embed.reshape(1, old_h, old_w, embed_dim)
                    # Permute to [1, dim, old_h, old_w] for interpolation
                    old_embed_spatial = old_embed_spatial.permute(0, 3, 1, 2)
                    
                    # Interpolate to new size
                    new_embed_spatial = torch.nn.functional.interpolate(
                        old_embed_spatial,
                        size=(new_h, new_w),
                        mode='bilinear',
                        align_corners=False
                    )
                    
                    # Reshape back: [1, dim, new_h, new_w] -> [1, new_num_patches, dim]
                    new_embed_spatial = new_embed_spatial.permute(0, 2, 3, 1)
                    state_dict[key] = new_embed_spatial.reshape(1, new_num_patches, embed_dim)
        
        # Load the state dict (with resized embeddings if needed)
        model.load_state_dict(state_dict, strict=False)
        
        # Load feature projection layer if available
        if 'model_state_dict' in checkpoint:
            if 'feature_proj_state_dict' in checkpoint and descriptor_extractor.feature_proj is not None:
                descriptor_extractor.feature_proj.load_state_dict(checkpoint['feature_proj_state_dict'])
                logging.info("Loaded feature projection layer from checkpoint")
        
        # Try to load projection from separate checkpoint if --projection_checkpoint is specified
        if args.projection_checkpoint is not None and descriptor_extractor.feature_proj is not None:
            try:
                proj_checkpoint = torch.load(args.projection_checkpoint, map_location=device, weights_only=True)
                if 'feature_proj_state_dict' in proj_checkpoint:
                    descriptor_extractor.feature_proj.load_state_dict(proj_checkpoint['feature_proj_state_dict'])
                    logging.info(f"Loaded projection layer from separate checkpoint: {args.projection_checkpoint}")
                else:
                    logging.warning(f"No 'feature_proj_state_dict' found in {args.projection_checkpoint}")
            except Exception as e:
                logging.error(f"Failed to load projection checkpoint from {args.projection_checkpoint}: {str(e)}")
        
        # If no projection loaded yet, try to auto-detect separate projection file
        elif descriptor_extractor.feature_proj is not None and 'feature_proj_state_dict' not in checkpoint:
            auto_proj_path = args.checkpoint_dir.replace('model_weights', 'projection_weights')
            if os.path.exists(auto_proj_path):
                try:
                    proj_checkpoint = torch.load(auto_proj_path, map_location=device, weights_only=True)
                    if 'feature_proj_state_dict' in proj_checkpoint:
                        descriptor_extractor.feature_proj.load_state_dict(proj_checkpoint['feature_proj_state_dict'])
                        logging.info(f"Auto-loaded projection layer from: {auto_proj_path}")
                except Exception as e:
                    logging.warning(f"Failed to auto-load projection from {auto_proj_path}: {str(e)}")
        
        logging.info(f"Successfully loaded checkpoint from {args.checkpoint_dir}")
    except Exception as e:
        logging.error(f"Failed to load checkpoint from {args.checkpoint_dir}: {str(e)}")
        sys.exit(1)
    
    logging.info("Starting evaluation...")
    processed_test, pred_json_test, gt_json_test = {}, {}, {}
    test_losses = []
    
    model.eval()
    if descriptor_extractor.feature_proj is not None:
        descriptor_extractor.feature_proj.eval()
    
    for batch_idx, batch in enumerate(tqdm(test_dataloader, desc="Evaluation")):
        with torch.no_grad():
            if batch is None:
                continue
            
            DEST_descriptors, DEST_img_feats = descriptor_extractor.get_DEST_descriptors(batch)
            SOURCE_descriptors, SOURCE_img_feats = descriptor_extractor.get_SOURCE_descriptors(batch)
            
            is_visible_GT = batch['is_visible']
            POS_mask_position_GT = batch['POS_mask_position']
            
            similarities, pred_masks_idx, pred_mask, loss, top5_masks = model(
                SOURCE_descriptors, DEST_descriptors, 
                SOURCE_img_feats, DEST_img_feats, 
                batch['POS_mask_position'], batch['is_visible'],
                batch['DEST_SAM_masks'], test_mode=True
            )
            
            pred_mask = pred_mask.squeeze().detach().cpu().numpy()
            confidence = similarities.detach().cpu().numpy()
            
            if loss is not None:
                test_losses.append(loss.item())
            
            pred_json_test, gt_json_test = add_to_json(
                test_dataset_for_eval, batch['pair_idx'], 
                pred_mask, confidence,
                processed_test, pred_json_test, gt_json_test
            )
    
    # Calculate average test loss if available
    if len(test_losses) > 0:
        test_loss_mean = float(sum(test_losses) / len(test_losses))
        logging.info(f"Average test loss: {test_loss_mean:.6f}")
    else:
        test_loss_mean = None
        logging.info("No test loss values collected")

    logging.info("Computing evaluation metrics...")
    final_json_gt = {
        "version": "xx",
        "challenge": "xx",
        "annotations": gt_json_test
    }

    aggregated_metrics, per_observation_metrics = evaluate(gt_json_test, pred_json_test, args.reverse)
    
    # Log all aggregated metrics
    logging.info("Evaluation metrics (aggregated):")
    for metric_name, metric_value in aggregated_metrics.items():
        logging.info(f"  {metric_name}: {metric_value:.6f}")
    
    # Log per-observation statistics
    logging.info("Per-observation statistics:")
    logging.info(f"  Total observations: {len(per_observation_metrics['iou_per_obs'])}")
    if len(per_observation_metrics['iou_per_obs']) > 0:
        iou_std = float(np.std(per_observation_metrics['iou_per_obs']))
        logging.info(f"  IoU std: {iou_std:.6f}")

    # Save metrics with both aggregated and per-observation metrics
    metrics_save_path = os.path.join(output_dir, f'results_metrics_{run_folder}.json')
    metrics_results = {
        "exp_name": args.exp_name,
        "run_folder": run_folder,
        "args": vars(args),
        "checkpoint_path": args.checkpoint_dir,
        "test_loss": test_loss_mean,
        "aggregated_metrics": aggregated_metrics,
        "per_observation_metrics": per_observation_metrics,
        "pipeline": "precomputed_features"
    }
    save_json(metrics_results, metrics_save_path, "evaluation metrics")

    # Saving the json with the results
    logging.info("Saving prediction and ground truth JSON files...")
    if args.reverse:
        final_json = {'exo-ego': {'results': pred_json_test}}
        assert "exo-ego" in final_json
        preds = final_json["exo-ego"]

        assert type(preds) == type({})
        for key in ["results"]:
            assert key in preds.keys()

        save_path = os.path.join(output_dir, 'exo2ego_predictions_' + args.exp_name + '.json')
        save_path_gt = os.path.join(output_dir, 'exo2egoGT.json')
    else:
        final_json = {'ego-exo': {'results': pred_json_test}}
        assert "ego-exo" in final_json
        preds = final_json["ego-exo"]

        assert type(preds) == type({})
        for key in ["results"]:
            assert key in preds.keys()
        
        save_path = os.path.join(output_dir, 'ego2exo_predictions_' + args.exp_name + '.json')
        save_path_gt = os.path.join(output_dir, 'ego2exoGT.json')

    save_json(convert_ndarray(final_json), save_path, "predictions JSON")
    save_json(convert_ndarray(final_json_gt), save_path_gt, "ground truth JSON")
    
    # Final summary
    logging.info("=" * 80)
    logging.info("Evaluation completed successfully!")
    logging.info(f"Pipeline: PRECOMPUTED FEATURES")
    if 'iou' in aggregated_metrics:
        logging.info(f"Best IoU: {aggregated_metrics['iou']:.6f}")
    else:
        logging.info("Best IoU: N/A")
    logging.info(f"All results saved to: {output_dir}")
    logging.info(f"Metrics saved to: {metrics_save_path}")
    logging.info(f"Predictions saved to: {save_path}")
    logging.info(f"Ground truth saved to: {save_path_gt}")
    logging.info("=" * 80)

